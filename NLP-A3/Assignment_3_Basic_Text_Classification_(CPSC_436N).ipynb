{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5842d2f"
   },
   "source": [
    "# Assignment 3: Text Classification (CPSC 436N)\n",
    " \n",
    "\n",
    "* Follow the instructions in this notebook to develop a Bag Of Word (BOW) (textbook Fig 4.1), and a word embedding-based classifiers for text classification. \n",
    "\n",
    "* Test these implemented classifiers on subsets of the 20newsgroup corpus, and analyze the errors and successes of each model compared to the other.\n",
    "\n",
    "* In this assigment, you will use sklearn (that you should have already used extensively in either cpsc330 or cpsc340) and spacy that you briefly saw in Assignment 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db16ab35"
   },
   "source": [
    "## 1. Install Spacy and pipelines that will be used in later steps:\n",
    "\n",
    "#### Install Spacy v3.0: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bc1p1lcRXPAv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Users\\Amy' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install spacy==3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igaf9WsjXeLu"
   },
   "source": [
    "#### Download and install the trained English pipeline ([en_core_web_lg](https://spacy.io/models/en) (https://spacy.io/models/en)) provided by Spacy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U1C3Ur4MXsI9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy George\\.virtualenvs\\swtheart_hw7\\Scripts\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b548499"
   },
   "source": [
    "## 2. Load dataset (20newsgroups)\n",
    "\n",
    "For this assignment, we train and test classification models on the 20newsgroups corpus, which can be easily fetched by sklearn. This dataset comprises around 18000 newsgroups posts on 20 topics and has been splited into two subsets by sklearn for model training and testing. \n",
    "\n",
    "To ensure this assignment is manageable and won't take too long for training and inference, we will use the subset of 20newsgroups only covering samples belonging to either one of the two classes (like '__talk.politics.misc__', '__talk.religion.misc__' used below). With this setting, we will perform a  binary classification (instead of multiclass classification). \n",
    "\n",
    "Please **read carefully** the two links below which provide details about the 20newsgroups corpus and how to load and process it with sklearn:\n",
    "\n",
    "* http://qwone.com/~jason/20Newsgroups/\n",
    "* https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dc4d1f2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=['talk.politics.misc','talk.religion.misc'])\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=['talk.politics.misc','talk.religion.misc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al2PNNY1M94C"
   },
   "source": [
    "**Q1:** Once the data has been loaded, please look at the testing set (__newsgroups_test__) and answer these questions:\n",
    "- who did send the shortest message? \n",
    "- Who did send the longest message? \n",
    "- What are the labels of the shortest/longest message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64W_1-AekQdp",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#### SOLUTION Q1  ######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eae9dd1"
   },
   "source": [
    "## 3. Create your own tokenizer with Spacy:\n",
    "\n",
    "Tokenization is a critical preprocessing step when we work with text data. What [tokenization](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/) does is separating input text into tokens, which can be either words, characters, or subwords, and further processing these tokens to reduce the noise caused by informal expressions, typos or common but meaningless words contained in text.\n",
    "\n",
    "There are different ways to specialize your tokenization strategy. Some typical steps include __lowercasing__, __stop words and punctuations removing__, __lemmatization__ and __filtering tokens with part-of-speech tagging__.\n",
    "\n",
    "**Q2:** In the code cell below, please complete your tokenization function with Spacy (named as __spacy_tokenizer__). It shall cover:\n",
    "\n",
    "* Lowercasing\n",
    "\n",
    "* Removing stop words\n",
    "\n",
    "* Removing punctuations\n",
    "\n",
    "* Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b47c6ae3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#import libs needed for tokenization.\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import string\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')  # Load trained English pipeline \"en_core_web_lg\".\n",
    "punctuations = string.punctuation  # Get the list of punctuation.\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS  # Get the list of stop words identified in the loaded language model.\n",
    "\n",
    "# Creating your own tokenizer function with functions built in Spacy.\n",
    "def spacy_tokenizer(doc):\n",
    "\n",
    "    tokens = nlp(doc)  # Splits the doc into tokens and applies the loaded pipeline \n",
    "    #(create all linguistic annotations for the doc, including POS etc).\n",
    "\n",
    "    ######## FOR SOLUTION Q2 ########\n",
    "    # Lemmatizing each token and converting each token into lowercase. You can use spacy Token.lemma_\n",
    "    \n",
    "\n",
    "    # Removing stop words and punctuations\n",
    "    \n",
    "    ######## FOR SOLUTION Q2 ########\n",
    "    \n",
    "    return tokens  # return preprocessed list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c1cdae7"
   },
   "source": [
    "## 4. Build the pipeline for BOW classification model\n",
    "\n",
    "Now let's design the pipeline for the text classification model with sklearn!\n",
    "\n",
    "We first start with BOW classifier, the overall pipeline for it should contain:\n",
    "\n",
    "* A BOW vectorizer applying the tokenizer implemented above\n",
    "\n",
    "* A classifier, which should be set to logistic regression for now\n",
    "\n",
    "**Q3:** Please complete the code in the cell below by following the comments.\n",
    "\n",
    "<br>\n",
    "\n",
    "If needed, please read the sklearn guide:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "to learn how to use CountVectorizer to obtain BOW vectors with customized tokenizer, and:\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "to learn how to use Pipeline object to implement classification models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f08cf39",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "######## FOR SOLUTION Q4 ########\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))  # Create BOW vectorizer object.\n",
    "# SOLUTION Q4 (a): to change the BOW vectorizer to include both unigram+bigram setting: \n",
    "\n",
    "classifier = LogisticRegression()  # Create Logistic Regression classifier object.\n",
    "# SOLUTION Q4 (b): to change the classifier to NaiveBayes\n",
    "\n",
    "\n",
    "\n",
    "######## FOR SOLUTION Q3 ########\n",
    "\n",
    "# Create pipeline for BOW classfier.\n",
    "pipe = Pipeline([('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4805075b"
   },
   "source": [
    "Now let's train the model on the training set obtained from 20newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOCdNLWWcQIX",
    "outputId": "beb55c84-df5c-44f2-ce94-cd2b4cdfb079",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# >>> pipe.fit(X_train, y_train)\n",
    "pipe.fit(newsgroups_train.data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5szy3bROcYTF"
   },
   "source": [
    "Now let's evaluate our BOW classifier's performance on the testing set obtained from 20newsgroup.\n",
    "\n",
    "As an example, here we only compute [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) as the evaluation metric, more sophisticated evaluation metrics or techniques are needed for deeper model analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f38e6d29",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = pipe.predict(newsgroups_test.data)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(newsgroups_test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtGM3zdNU13g"
   },
   "source": [
    "**Q4:** In the code above, we apply the BOW vectorizer with unigram setting, now please change the BOW vectorizer with unigram+bigram setting and report the accuracy you get.\n",
    "\n",
    "Now please try Naive Bayes as the classifier with BOW vectorizer including unigram and unigram+bigram, then report the accuracies you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "799bc5d3"
   },
   "source": [
    "## 5. Build the pipeline for the word embedding-based classification model \n",
    "\n",
    "The pipeline of BOW classifier implemeted above consists of two components: BOW vectorizer and the classifier (LogReg or NB). In that scenario, in sklearn our customized tokenizer could be called together with the BOW vectorizer.\n",
    "\n",
    "This new classifier will use a distributed representation of the input document as the average of the embeddings of the words contained in the document. To deal with this very different input we will use spacy. And this requires that the preprocessing of text (tokenization) and the vectorizing of documents as the average of word embeddings should be two separate steps. Thus, the pipeline for this classifier will include three components:\n",
    "\n",
    "* a preprocessing component \n",
    "\n",
    "* a document vectorization component\n",
    "\n",
    "* a classifier\n",
    "\n",
    "We will keep the classifier as Logistic Regression, but we need to implement the classes for the preprocessing and document vectorization components (named as __Preprocessing_CMPT__ and __SpacyWordEmb_CMPT__ in the code cell below) and use them for the sklearn pipeline construction. Please note that we need to implement SpacyWordEmb_CMPT with word embeddings provided by \"en_core_web_lg\" in Spacy.\n",
    "\n",
    "**INCLUDE IN CANVAS ASSIGNMENT as Q5:** Please whre requested add comments to the transform functions for Preprocessing_CMPT and SpacyWordEmb_CMPT in the code cell below. The comments should explain the best you can what the code is doing and why.\n",
    "\n",
    "Notice that smoothly chain up components as a pipeline, each component should have a sklearn __transform( )__ and  __fit( )__ function:\n",
    "\n",
    "* __transform( )__: transforming the input data into the format ready to pass to the next component.\n",
    "\n",
    "* __fit( )__: learning/calculating the parameters from the training data. The parameters learned by using the training data will help us to transform our test data. **It will not be used** if there is no need to learn parameters from the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cedf2f46",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import libs and classes needed for component construction.\n",
    "from sklearn.base import BaseEstimator \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# define the class of the preprocessing component\n",
    "class Preprocessing_CMPT(BaseEstimator): \n",
    "    def transform(self, X, **transform_params): # the function actually performs the preprocessing, X contains all samples in the input corpus.\n",
    "        return [spacy_tokenizer(text) for text in X] # ADD COMMENT\n",
    "        \n",
    "\n",
    "    # in this case, we don't need to calculate values for later scaling, so fit() is doing nothing but just return self.\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "# define the class of the document vectorization component based on word embeddings\n",
    "class SpacyWordEmb_CMPT(BaseEstimator): \n",
    "    def __init__(self, nlp): # define the language model and dimension of word embeddings we want to use in this component.\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "    \n",
    "    def transform(self, X): \n",
    "    # this function  converts documents into the average of their word embeddings, X contains all documents in the input corpus.\n",
    "        X_str = []\n",
    "        for text in X:\n",
    "            X_str.append(' '.join(text)) # ADD COMMENT\n",
    "        return [self.nlp(text).vector #ADD COMMENT see https://spacy.io/api/doc#vector\n",
    "                for text in X_str] \n",
    "\n",
    "    # in this case, fit() is doing nothing as well.           \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')   # Load the pretrained english pipeline \"en_core_web_lg\"\n",
    "\n",
    "# Create pipeline for the word embedding-based classfier.\n",
    "pipe = Pipeline([(\"preprocessing\", Preprocessing_CMPT()),\n",
    "                ('encoding', SpacyWordEmb_CMPT(nlp)),\n",
    "                (\"classifier\", classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8v7LdADuG6b"
   },
   "source": [
    "Now let's train this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c76be14f",
    "outputId": "fa170af9-7188-4300-d3e0-a2d7490ed5bd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(newsgroups_train.data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Muo8_fphuMba"
   },
   "source": [
    "And do prediction on the testing set and evaluate the performance of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2f9c011",
    "outputId": "6fa5d566-b70b-414c-c5e8-6a6f1761a56d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(newsgroups_test.data)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(newsgroups_test.target, predicted))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3: Basic Text Classification (CPSC 436N).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}